{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Stats in Python and Notebooks - Testing\n",
    "\n",
    "This type of file is called a notebook. They are commonly used in data science. \n",
    "These notebooks allow us to write code, execute code, and add webpage style text all in one spot.\n",
    "Each 'block' is called a cell. Markdown cells are like this one, they have text. Code cells are like the one below, that's where our code goes.\n",
    "The little play sign next to each code cell is how we run (execute) that bit of code. Any errors or results will print below that cell. \n",
    "\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "The imports that we have here allow us to load external code packages that will help us do stuff. We can load things that will help us draw charts, make models, do math, etc...\n",
    "\n",
    "There's actually relatively little \"stuff\" that is part of a programming language like Python. Each of these libraries is a collection of functionality made by someone else that we can import into our programs and use as if we wrote it. For example, right now we are importing:\n",
    "<ul>\n",
    "<li>Two from the maker of the text \"thinkstats/plot\", these provide us with some useful stats calculations as well as some easy graphing. \n",
    "<li>Pandas, which gives us dataframes - a data structure (thing that holds data) that is a kind of python-spreadsheet. \n",
    "<li>Numpy, which gives us some other common mathmatical functionality. \n",
    "</ul>\n",
    "\n",
    "Normally, when we want to do something different we'll need to load some of these libraries, which will give us the toolset without forcing us to make things from scratch. When we installed Anaconda, a bunch of these libraries were installed by default, these are the ones that we can import. There are other ones that we'll need to install before we can use them. Installing things can be done in a few ways, such as:\n",
    "<ul>\n",
    "<li> Running \"conda install <name>\" from a terminal. \n",
    "<li> Running \"pip install <name>\" from a terminal. (We'll install pip now)\n",
    "<li> The GUI in Anaconda. \n",
    "<li> Using \"!pip install <name>\" in code. (This is useful later on in the ML class, when we use Colab)\n",
    "</ul>\n",
    "\n",
    "Note that this stuff can differ, in annoying ways, from system to system - especially if you're using Windows. On a Mac it is relatively easier. You may get some weird errors or issues that need to be investigated and dealt with individually. \n",
    "\n",
    "### Environments\n",
    "\n",
    "One thing to pay attention to is that we can have different environments in which your python code can run. These environments can differ because they can each hold different stuff, such as different versions of python and different sets of installed libraries. For us, having different environemnts is not really very useful, if someone is developing real software it can be important because someone can build and test their program in one environment with a specific set of stuff, and know that as long as that environment is kept the same, this code can be used anywhere and should work fine. Things like updates to the kernel, changes to libraries (e.g. even a name change to a function will break things), or updates that break other things won't break our program because it is in one specific environment; if we wanted to update something, we can test it and fix issues in a controlled way. This is pretty analogous to something like the Windows or Mac updates pushed out to your computer, once in a while an update will break some installed program in some way; large companies generally stop these updates from happening automatically so they can test the changes, then they push out the update once they are clear it doesn't cause an issue. Here, developers can not update new changes to the environment until they have a chance to check and see that nothing breaks. \n",
    "\n",
    "If we are working on different types of projects, we can also have environments setup with all of their libraries installed and configured. For example, developing neural networks to do image recognition generally uses big sets of libraries that are useful for that specific application. We might have one environment for this, and another that has all of the stuff needed for out other work where we are processing text. I created a backup environment last year before I installed some libraries that might interact with others and cause issues. \n",
    "\n",
    "For us, we need to set the environment when we run code. We can see the environment in the upper right of the editor window. If we run code and nothing works, and we get messages that common stuff is missing, this is a likely culprit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The pound sign means a comment, you can write free text here that will be ignored when you run some code. \n",
    "#Import some libraries - these packages have usefull stuff that we can use to make out lives easier....\n",
    "#The 'as pd' thing just gives us a nickname, we'll use that pandas name often, and pd is easier to type\n",
    "#Using pandas as pd isn't required, but is almost universally common. Same with numpy. \n",
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We'll load some data in from a CSV flie to get started. The data will get loaded into something called a dataframe, which is a type of data structure - or programming thing that holds data. Functionally, it is basically like turning the data in that file into a spreadsheet in our program, we can then manipulate that spreadsheet to do what we want. Once the data is loaded, the head function lets us take a peek at the first 5 rows. \n",
    "\n",
    "<b>Look at the data - what's cetegorical, what is numerical?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'pd' that we imported before gives us something called a dataframe\n",
    "#The df as a name for the dataframe is a common standard, it can be anything\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables in Memory\n",
    "\n",
    "The basic process of writing a computer program is to create variables (objects) that exist in the computer's memory, then \"do things\" to those varaibles in memory to make them into what we want for a result. For example, we may have \"a = 5\" and \"b = 7\" as two variables, and we may have another step that says \"a * b\" to get 35 - our final result. \n",
    "\n",
    "In the example above we are placing that entire 2D table into a varaible called df. We then use the function \"head()\" to get a preview of the first 5 rows. Using \"print\" type of statements such as that are a good way to check what is in our variables as we proceed. We can also use VS Code's Variables window to see a list of our variables. \n",
    "\n",
    "This can often be helpful, especially as we become acustomed to programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Columns and Info\n",
    "\n",
    "Each variable, or feature as they are called in data science is stored in a column of our spreadsheet-like dataframe. In addition to their value (in this case df holds that table of data), variables are objects that have a certain type - i.e. here \"df\" is a dataframe type object. Objects, in addition to their data, can hold other values - attributes, as well as other \"capabilities\" - functions. \n",
    "\n",
    "The .columns below is an attribute - columns is a list of the column names of the dataframe. Here we are asking for what value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what columns we have\n",
    "#This is calling a value that exists inside our dataframe, so it is just \"object.value\".\n",
    "#You'll learn more on this in the programming class.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see a little more details on this... Info() is a function in the dataframe class, so any dataframe can have it called. Here when we call \"info()\" it will run some code on the object df that will generate the info printed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Columns\n",
    "\n",
    "We can 'grab' a column by using the name (in a couple of ways), or the position (0,1,2,3...) Think about how this relates to the stuff above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works, we'll usually avoid it because it is not quite as clear\n",
    "df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This also works, and is sometimes usefull if you're doing things like looping through data\n",
    "#It is probably more confusing in most cases though, so we won't use it.\n",
    "#Survived is the second column, and in programming we (almost) always start counting at 0. So we are getting column #1\n",
    "df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is probably the most simple way, and this is what I'll try to use all the time\n",
    "#Unless specified explicity, you can use whatever you want (in general, not just this)\n",
    "#This will be the easiest to keep straight, I think\n",
    "df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "<ul>\n",
    "<li> Print the \"embarked\" column.\n",
    "<li> Put the \"fare\" column into its own variable, then print the head() of that new object\n",
    "<li> Print two columns at once - i.e. rather than slicing out one column, do two at the same time. Use \"Survived\" and \"Age\". This might need some Googling. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Embarked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = df[\"Fare\"]\n",
    "fare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge - print multiple columns. Survived and Age!\n",
    "#You may need to Google, think about what to Google and try to implement what you find\n",
    "df[[\"Survived\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Stats\n",
    "\n",
    "We can get some basic statistics on each column, to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This time we're calling a function that exists inside our dataframe, so it is \"object.function()\". Note the brackets.\n",
    "# Same deal, learn this in the other class.\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rotate, or transpose, the result if we want to look at it the other way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Statistics\n",
    "\n",
    "In addition to the bulk calculations done with describe, we can also ask for specific values. (Here I use print statements to get more than one line of text, and add in some labels as well)\n",
    "\n",
    "Note: Think about the variables generated in this code. Look especially at line 2. What is an object? What is \"name\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_col = df[\"Fare\"]\n",
    "print(\"Some Basic Statistics for\", sample_col.name)\n",
    "\n",
    "print(\"Mean:\", sample_col.mean())\n",
    "print(\"Median:\", sample_col.median())\n",
    "print(\"Variance\", sample_col.var())\n",
    "print(\"Stan. Dev.\", sample_col.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "Print basic statistics for the object you made above with the \"fare\" data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some Basic Statistics for\", fare.name)\n",
    "\n",
    "print(\"Mean:\", fare.mean())\n",
    "print(\"Median:\", fare.median())\n",
    "print(\"Variance\", fare.var())\n",
    "print(\"Stan. Dev.\", fare.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Counts\n",
    "\n",
    "In programming talk, null means nothing (not 0, or blank, literally nothing). So, we are checking to see which columns have a bunch of missing values. In the table above the NaN (Not a Number) represents the same concept. \n",
    "\n",
    "In computer-speak, true/false values are called Boolean variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also chain function calls together, to do more stuff. The code below will take the \"isnull\" results from above, and the sum() function will add them up. Recall: true/false is 1/0 in computer-speak, so we can just add all the ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to tally it up and count how many NULLs we have. \n",
    "#It is simple (one line) - try to Google and implement. \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Count\n",
    "\n",
    "When we have more than just true/false data, we can use a different function to add up the results for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many people lived? Look at the Survived column and tally it up.\n",
    "#In programming 0/1 for true false is common. 0=false, 1=true. Virtually always. \n",
    "df[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts can also help us figure out if there is any junk.\n",
    "df[\"Cabin\"].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try It\n",
    "\n",
    "Count the number of null values we have in the \"age\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe Manipulation\n",
    "\n",
    "The cabin looks like it is just their room number. I think this is useless, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll get rid of it.\n",
    "df.drop(columns=[\"Cabin\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Variable Stats\n",
    "\n",
    "I'm a shallow capitalist, how much did people pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Fare\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More looking at who paid what. Is there a better way to do this? I threw in that \"round\" function to get rid of messy decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df[\"Fare\"]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a really shallow capitalist, how much did people in different classes pay?\n",
    "This will take a few boxes....\n",
    "First, what classes are there? In order this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pclass\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Segmentation\n",
    "\n",
    "We commonly want to separate large datasets into subsets, for assorted reasons. We can think of slicing a dataframe in 2 ways:\n",
    "<ul>\n",
    "<li> By Columns - vertically. We can slice some columns of data out. \n",
    "<li> Br Rows - horizontally. We can slice out rows of data that meet certain conditions. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make separate dataframes, one for each class of passenger\n",
    "#Try one first, and check visually that we did it right. How do you check?\n",
    "df_1 = df[df[\"Pclass\"] == 1]\n",
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the other two\n",
    "df_2 = df[df[\"Pclass\"]==2]\n",
    "df_3 = df[df[\"Pclass\"]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look\n",
    "#What happens?\n",
    "df_1[\"Fare\"].describe()\n",
    "df_2[\"Fare\"].describe()\n",
    "df_3[\"Fare\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm wrapping all these in print statements. This'll force them all to print. \n",
    "#This is kind of ugly, but it'll do the job here. \n",
    "print(df_1[\"Fare\"].describe())\n",
    "print(df_2[\"Fare\"].describe())\n",
    "print(df_3[\"Fare\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Draw Any Inferences?\n",
    "What kind of conclusions or insights can you draw here? Any changes to the data that might make this better? Is there any more insightfull way you could look at this same data?\n",
    "\n",
    "Let's ask a few more questions and see if we can find answers...\n",
    "\n",
    "<b>How much better off were high class passengers to survive?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "\n",
    "#Note: for the stuff below, there's not necissarily one way to do things, especially as the data gets more complex.\n",
    "#When we're exploring the data there's usually several ways to look for the insights, sometimes some are better than others.\n",
    "#What makes the most sense depends on the data, you'll get better at deciding with experience.\n",
    "\n",
    "high_survive = df_1[\"Survived\"].sum()\n",
    "low_survive = df_3[\"Survived\"].sum()\n",
    "\n",
    "print(\"High Survive Rate:\", high_survive/df_1.shape[0])\n",
    "print(\"Low Survive Rate:\", low_survive/df_3.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What was the mean fare of survivors vs dead DiCaprios?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "surv_fare = df[df[\"Survived\"] == 1]\n",
    "print(\"Survived:\", surv_fare[\"Fare\"].mean())\n",
    "\n",
    "dead_fare = df[df[\"Survived\"] == 0]\n",
    "print(\"Dead:\", dead_fare[\"Fare\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Women and children first! We need to make sure they survive!!!!! Does that hold up? Try to examine it.</b>\n",
    "\n",
    "Think about what you are doing to split the dataset, and if there are any scenarios that might be an issue with how you've done things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "women = df[df[\"Sex\"] == \"female\"]\n",
    "men = df[df[\"Sex\"] == \"male\"]\n",
    "kids = df[df[\"Age\"]< 18]\n",
    "\n",
    "w_surv = women[women[\"Survived\"] == 1]\n",
    "print(\"Women Survival Rate:\", w_surv.shape[0]/women.shape[0])\n",
    "\n",
    "m_surv = men[men[\"Survived\"] == 1]\n",
    "print(\"Men Survival Rate:\", m_surv.shape[0]/men.shape[0])\n",
    "\n",
    "k_surv = kids[kids[\"Survived\"] == 1]\n",
    "print(\"Men Survival Rate:\", k_surv.shape[0]/kids.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Away...\n",
    "\n",
    "Try to do a couple of things on your own, for practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "data_path = \"data/HeightWeight.csv\"\n",
    "df2 = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview/explore the data a bit. \n",
    "# Get an idea of what is in \n",
    "\n",
    "#Something like\n",
    "#df2.describe(include=\"all\")\n",
    "#df2.info()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distributions for Male and Female heights and weights. \n",
    "# 1. Compare the difference between mean and median for both height datasets - what does that reveal?\n",
    "# 2. Which of the weight datasets seems to be more \"compact\" vs. more \"spread out\"?\n",
    "# 3. Look at the pandas documentation: https://pandas.pydata.org/docs/reference/index.html and try to do something else to the dataframe, \n",
    "# preferably something you cna make sense of. This is just an intro to reading documentation - you can pick anything. Don't spend an eternity on this, just try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Too Easy???..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you're super fast, you can start looking at this stuff below.\n",
    "#We'll cover this next week in detail, so if you don't get it, no big deal. \n",
    "\n",
    "#We can draw pretty pictures!\n",
    "#Look up a Python dictionary and make sense of the output below.\n",
    "hist = thinkstats2.Hist(np.floor(df[\"Fare\"]), label='Fare')\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.Hist(hist)\n",
    "thinkplot.Config(xlabel='fare', ylabel='count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "375199ce1c3e0e0b1284d55f321bc02a385776e0a0563a4ff2352394f0bcc9ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
